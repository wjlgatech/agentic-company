<div align="center">
  <img src="assets/mascot.png" alt="Agenticom Golden Retriever" width="200">
  <h1>Agenticom: Multi-Agent Team Orchestration</h1>
  <p>
    <img src="https://img.shields.io/badge/python-â‰¥3.10-blue" alt="Python">
    <img src="https://img.shields.io/badge/license-MIT-green" alt="License">
    <img src="https://img.shields.io/badge/tests-849+%20passed-brightgreen" alt="Tests">
    <img src="https://img.shields.io/badge/test__lines-6,000+-blue" alt="Test Lines">
    <img src="https://img.shields.io/badge/status-alpha-orange" alt="Status">
  </p>
</div>

ğŸ• **Agenticom** is a multi-agent workflow orchestration framework inspired by [Antfarm](https://github.com/snarktank/antfarm).

âš¡ï¸ **One agent makes mistakes. Five agents cross-verify.**

## ğŸ“¢ News

- **2026-02-14** ğŸ§  **ADAPTIVE MEMORY COMPLETE**: Production-ready lesson learning system with LLM extraction + human curation
  > [Phase 2 Details](MONITORING_ADAPTIVE_MEMORY.md) â€¢ 1,695 lines backend â€¢ 6,000+ lines tests
- **2026-02-14** ğŸ“Š **COMPREHENSIVE MONITORING**: Full observability framework for measuring memory effectiveness
  > [Metrics Guide](MONITORING_ADAPTIVE_MEMORY.md) â€¢ Leading + lagging indicators â€¢ Root cause analysis â€¢ Automated alerting
- **2026-02-14** ğŸ¯ **STAGE TRACKING**: Real-time workflow stage visualization (Plan â†’ Implement â†’ Verify â†’ Test â†’ Review)
  > [Phase 1 Details](PHASE1_COMPLETE_STAGE_TRACKING.md) â€¢ Timestamps + artifacts per stage â€¢ Auto-detection from step IDs
- **2026-02-14** âœ… **30+ REAL TESTS**: Comprehensive test suite with actual LLM calls, file I/O, and calculations
  > Unit + integration + real-world + stress tests â€¢ 849+ tests across 20 test files
- **2026-02-11** ğŸ¢ **7 ENTERPRISE WORKFLOWS**: Due diligence, compliance, patents, security, churn, grants, incidents!
- **2026-02-11** ğŸ”§ **UPGRADE**: Dynamic role resolution - any custom agent role now auto-maps to base types
- **2026-02-11** ğŸ† **V3 ASSESSMENT: â­â­â­â­â­ (5/5)** - Independent testing confirms all workflows working perfectly!
- **2026-02-11** ğŸ› **CRITICAL FIX**: Multi-step workflow variable substitution now works!
- **2026-02-11** ğŸ§  **NEW: PromptEngineer** - Automatic prompt improvement using Anthropic's best practices
- **2026-02-11** ğŸ”Œ **NEW: MCP Integration** - Connect workflow tools to real MCP servers (PubMed, Ahrefs, etc.)
- **2026-02-11** ğŸ‰ Added Web Dashboard + Golden Retriever mascot
- **2026-02-11** âœ¨ Core features verified with stress tests
- **2026-02-11** ğŸš€ Initial release with OpenClaw + Nanobot skill integrations

## âš ï¸ Current Status: Alpha Framework

**Agenticom is a FRAMEWORK, not a turnkey product.** It provides:

### âœ… What Works

| Feature | Status | Notes |
|---------|--------|-------|
| ğŸ›¡ï¸ **Guardrails** | âœ… Working | Content filter, rate limiter |
| ğŸ§  **Memory** | âœ… Working | Persistent remember/recall |
| ğŸ“ **Adaptive Memory** | âœ… Production | Lesson learning from workflows |
| ğŸ“Š **Memory Monitoring** | âœ… Production | Success rate tracking, alerting |
| ğŸ¯ **Stage Tracking** | âœ… Working | Real-time workflow visualization |
| âœ… **Approval Gates** | âœ… Working | Auto/Human/Hybrid patterns |
| ğŸ’¾ **Caching** | âœ… Working | LLM response cache |
| ğŸ“Š **Observability** | âœ… Working | Prometheus-style metrics |
| ğŸ“‹ **YAML Workflows** | âœ… Working | Parser loads bundled workflows |
| ğŸ–¥ï¸ **CLI** | âœ… Working | `workflow list`, `run --dry-run` |
| ğŸŒ **Dashboard** | âœ… Working | Visual workflow management |
| âš¡ **Multi-Backend** | âœ… Working | Ollama/Claude/GPT abstraction |
| ğŸ”Œ **MCP Integration** | âœ… Working | Connect to real MCP servers |
| ğŸ§  **Prompt Engineer** | âœ… Working | Auto-improve agent prompts |

### âš ï¸ What Requires MCP Server Connections

| Feature | Status | How to Enable |
|---------|--------|---------------|
| ğŸŒ **Web Search** | ğŸ”Œ MCP Ready | Connect Ahrefs or Similarweb MCP |
| ğŸ“š **Literature Search** | ğŸ”Œ MCP Ready | Connect PubMed MCP |
| ğŸ“± **Social Media** | ğŸ”Œ MCP Ready | Connect LunarCrush MCP |
| ğŸ“Š **Market Research** | ğŸ”Œ MCP Ready | Connect Harmonic or S&P Global MCP |
| ğŸ” **Competitor Analysis** | ğŸ”Œ MCP Ready | Connect Similarweb MCP |

**The bundled workflows now include MCP Tool Bridge integration.** Tools automatically resolve to real MCP servers when connected, or provide graceful fallback guidance when not.

<details>
<summary><b>ğŸ† View V3 Independent Assessment (5/5 Stars)</b></summary>

### Overall Rating: â­â­â­â­â­ (5/5) - FULLY FUNCTIONAL

| Metric | V1 (Pre-MCP) | V2 (Broken) | V3 (Fixed) | Trend |
|--------|--------------|-------------|------------|-------|
| **Multi-Step Coordination** | â­â­â­â­ | â­ | â­â­â­â­â­ | ğŸ“ˆ Fixed! |
| **Technical Execution** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | ğŸ“ˆ Restored |
| **Content Quality** | â­â­â­ | â­â­â­ | â­â­â­â­ | ğŸ“ˆ Better |
| **Overall** | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | ğŸ¯ Best! |

### Test Results (3/3 Workflows, 11/11 Steps):

**âœ… CAR-T Research (5 steps, 263s):**
- Validator caught fabricated citations
- Cross-verification working perfectly

**âœ… Software Development (4 steps, 157s):**
- Plan â†’ Code â†’ Tests â†’ Review all connected
- Reviewer found real bugs in generated code

**âœ… Marketing Campaign (2 steps, 103s):**
- Research â†’ Strategy properly integrated
- Pain points incorporated into personas

### Key Achievement:
> "From broken (V2) to excellent (V3) in one bug fix. The template substitution fix restored full functionality and the framework now delivers on its promise of multi-agent orchestration."

**Verdict:** âœ… **PRODUCTION READY** for software development and process automation

</details>

<details>
<summary><b>View Test Results (Critical Fixes Verified)</b></summary>

```
ğŸ§ª AGENTICOM CRITICAL FIX TEST SUITE
============================================================
TestYAMLParserFix:
  âœ… feature-dev.yaml loads correctly
  âœ… marketing-campaign.yaml loads correctly
  âœ… Parser preserves persona from prompt field
  âœ… Parser correctly uses 'id' for role mapping

TestCLIWorkflowExecution:
  âœ… CLI dry-run mode works
  âœ… CLI workflow run is not mocked
  âœ… CLI shows helpful error when no LLM backend

TestWorkflowListDiscovery:
  âœ… Workflow list discovers actual YAML files

TestAgentTeamExecution:
  âœ… Agent correctly requires executor
  âœ… AgentTeam has async run method

TestWorkflowExecutorWiring:
  âœ… load_workflow() correctly leaves executors unset
  âœ… load_workflow(auto_setup=True) correctly requires LLM backend
  âœ… load_ready_workflow() correctly requires LLM backend
  âœ… load_ready_workflow correctly exported

============================================================
RESULTS: 14/14 tests passed
```

**Critical Fixes Applied (2026-02-11):**
1. **YAML Parser**: Now correctly uses `id` field for role mapping (was using `role` which contained descriptions)
2. **CLI Execution**: Replaced mock `time.sleep()` with real workflow execution
3. **Error Handling**: Clear messages when LLM backend not configured

</details>

## ğŸ“¦ Install

**1-Click** (auto-detects OS, installs missing prerequisites, creates venv)

```bash
git clone https://github.com/wjlgatech/agentic-company.git
cd agentic-company && bash setup.sh
```

**Or with `make`:**

```bash
git clone https://github.com/wjlgatech/agentic-company.git
cd agentic-company

# For users (production use)
make install && .venv/bin/agenticom install

# For developers (includes pytest, ruff, mypy, black)
make dev
```

`setup.sh` handles everything automatically:
- Installs `python3`, `pip`, `venv`, `make`, `git` if missing (via Homebrew / apt / dnf / pacman)
- Creates a `.venv` virtual environment
- Installs the package and bundled workflows

## ğŸš€ Quick Start

**1. Configure LLM backend (required)**

```bash
# Option A: Ollama (FREE - local)
ollama serve && ollama pull llama3.2

# Option B: Claude
export ANTHROPIC_API_KEY=sk-ant-...

# Option C: GPT
export OPENAI_API_KEY=sk-...
```

**2. Preview a workflow (dry-run)**

```bash
# See what a workflow will do without executing
agenticom workflow run feature-dev "Add login button" --dry-run
```

**3. Run a workflow**

```bash
# Actually execute the workflow (requires LLM backend)
agenticom workflow run feature-dev "Add a hello world function"
```

**4. Open dashboard**

```bash
agenticom dashboard
```

> âš ï¸ **Note**: Workflows execute via LLM prompts. Tools like `web_search` and `social_api` in marketing workflows are **declarative** - you must implement actual tool execution for real-world use.

## ğŸŒ Web Dashboard

For non-technical users who prefer a visual interface:

```bash
agenticom dashboard
```

Opens at `http://localhost:8080` with:
- ğŸ“Š **Stats Overview** - Success rate, running, failed
- ğŸ¯ **Quick Start** - Run workflows from browser
- ğŸ“‹ **Kanban Board** - Visual pipeline view
- ğŸŒ™ **Dark Mode** - Auto-detect system preference

## ğŸ“‹ Workflows

### Core Workflows
| Workflow | Pipeline | Use for |
|----------|----------|---------|
| `feature-dev` | plan â†’ implement â†’ verify â†’ test â†’ review | Software development |
| `marketing-campaign` | discover â†’ analyze â†’ create â†’ outreach â†’ orchestrate | Go-to-market |

### ğŸ†• Enterprise Workflows (7 High-Impact Use Cases)

| Workflow | Pain Point | Time Saved | Cost Saved |
|----------|------------|------------|------------|
| `due-diligence` | M&A analysis takes 4-8 weeks | 4-6 weeks | $50K-200K |
| `compliance-audit` | Manual audits miss gaps | 2-4 weeks | $25K-75K |
| `patent-landscape` | Patent lawyers charge $500-1000/hr | 3-6 weeks | $30K-100K |
| `security-assessment` | Breaches cost $4.45M average | 2-4 weeks | $20K-50K |
| `churn-analysis` | 5-7% churn = millions lost | 1-2 weeks | $10K-30K |
| `grant-proposal` | 40-100 hrs per proposal | 40-60 hours | $5K-15K |
| `incident-postmortem` | Post-mortems take days | 4-8 hours | $2K-5K |

<details>
<summary><b>ğŸ“Š Enterprise Workflow Details</b></summary>

**1. M&A Due Diligence** (`due-diligence`)
- 5 agents: Financial Analyst â†’ Legal Reviewer â†’ Market Analyst â†’ Technical Assessor â†’ Deal Lead
- Produces: Investment recommendation with GO/NO-GO decision

**2. Regulatory Compliance Audit** (`compliance-audit`)
- 5 agents: Requirements Mapper â†’ Gap Analyst â†’ Risk Assessor â†’ Remediation Planner â†’ Documenter
- Produces: Audit-ready compliance report with remediation roadmap

**3. Patent Landscape Analysis** (`patent-landscape`)
- 5 agents: Patent Searcher â†’ Claim Analyst â†’ Landscape Mapper â†’ FTO Assessor â†’ IP Strategist
- Produces: Freedom-to-operate assessment with IP strategy

**4. Security Vulnerability Assessment** (`security-assessment`)
- 5 agents: Threat Modeler â†’ Vuln Scanner â†’ Risk Analyst â†’ Remediation Engineer â†’ Security Architect
- Produces: Executive security report with prioritized fixes

**5. Customer Churn Analysis** (`churn-analysis`)
- 5 agents: Data Analyst â†’ Customer Researcher â†’ Segment Strategist â†’ Retention Strategist â†’ CCO
- Produces: Retention playbooks with ROI projections

**6. Grant/RFP Proposal Writing** (`grant-proposal`)
- 5 agents: Requirements Analyst â†’ Research Synthesizer â†’ Proposal Architect â†’ Budget Specialist â†’ Writer
- Produces: Complete proposal draft ready for submission

**7. Incident Post-Mortem** (`incident-postmortem`)
- 5 agents: Timeline Analyst â†’ RCA Specialist â†’ Impact Assessor â†’ Prevention Engineer â†’ Author
- Produces: Blameless post-mortem with action items

</details>

## ğŸ¯ Real-World Examples

<details>
<summary><b>ğŸ  Real Estate Marketing Team</b></summary>

```
Use agenticom marketing-campaign to create a digital marketing strategy
for a luxury real estate agency in Miami targeting international buyers.

Include: buyer personas, competitor audit (Douglas Elliman, Compass, Sotheby's),
30-day content calendar, influencer outreach list, 90-day launch plan with KPIs.
```

</details>

<details>
<summary><b>ğŸ§¬ Biomedical Research Deep Dive</b></summary>

```
Use agenticom feature-dev to research CAR-T cell therapy resistance in solid tumors.

Scout literature (2020-2024), categorize resistance mechanisms, verify claims
against primary data, generate 5 novel hypotheses, write 15-page review article.
```

</details>

<details>
<summary><b>ğŸš€ Idea to Product with PMF</b></summary>

```
Use agenticom feature-dev to validate my startup idea: "An AI copilot for
freelance consultants that turns client calls into SOWs and invoices."

Research market, analyze competitors, design MVP, build financial model,
create go-to-market plan for first 100 customers.
```

</details>

<details>
<summary><b>ğŸ’¼ M&A Due Diligence (Enterprise)</b></summary>

```
Use agenticom due-diligence to analyze acquisition target "TechStartup Inc"
with $10M ARR, 40% growth, B2B SaaS in the HR tech space.

Conduct financial analysis, legal review, market assessment, technical audit,
and provide investment recommendation with valuation range.
```

</details>

<details>
<summary><b>ğŸ”’ Security Assessment (Enterprise)</b></summary>

```
Use agenticom security-assessment to audit our e-commerce platform
handling 100K daily transactions and PII of 2M users.

Threat model, vulnerability scan, risk analysis, remediation plan,
and board-ready security report with compliance mapping.
```

</details>

## ğŸ’ª Strengths & Limitations

<details>
<summary><b>View Capability Analysis</b></summary>

### âœ… Strengths (What Agenticom Excels At)

| Capability | Rating | Evidence |
|------------|--------|----------|
| **Multi-Step Orchestration** | â­â­â­â­â­ | 5-step workflows with cross-verification |
| **Template Flexibility** | â­â­â­â­â­ | Dynamic `{{step_outputs.X}}` substitution |
| **Role Extensibility** | â­â­â­â­â­ | 50+ role mappings + auto-pattern matching |
| **YAML Simplicity** | â­â­â­â­â­ | Non-developers can create workflows |
| **Fresh Context** | â­â­â­â­ | Prevents context bloat in long workflows |
| **Cross-Verification** | â­â­â­â­ | Agents catch each other's errors |

### âš ï¸ Limitations (Areas for Improvement)

| Limitation | Impact | Mitigation |
|------------|--------|------------|
| **No Real Data Access** | Medium | Connect MCP servers for live data |
| **LLM Hallucinations** | Medium | Validator agents catch obvious issues |
| **Sequential Only** | Low | Parallel steps planned for v2 |
| **No Memory Across Runs** | Low | Use memory module for persistence |

### ğŸ”§ Recent Upgrades

1. **Dynamic Role Resolution** (this release)
   - Custom roles auto-map to base types via pattern matching
   - No more "Unknown agent role" errors
   - Supports any role name ending in -analyst, -researcher, etc.

2. **Template Preprocessing** (this release)
   - Fixed `{{step_outputs.X}}` â†’ `{X}` conversion
   - Hyphenated step IDs work correctly
   - Multi-step workflows fully functional

</details>

## ğŸ¦ Use with OpenClaw

[OpenClaw](https://github.com/openclaw/openclaw) - Personal AI assistant for WhatsApp, Telegram, Slack, Discord.

```bash
cd agentic-company && bash setup.sh
```

Then tell your assistant: *"Use agenticom to build a marketing strategy for my SaaS"*

## ğŸˆ Use with Nanobot

[Nanobot](https://github.com/HKUDS/nanobot) - Ultra-lightweight personal AI assistant.

```bash
cd agentic-company && bash setup.sh
```

Then tell your assistant: *"Use agenticom feature-dev to research and design a mobile app"*

## ğŸ–¥ï¸ CLI Reference

| Command | Description |
|---------|-------------|
| `agenticom install` | Install bundled workflows |
| `agenticom workflow list` | List available workflows |
| `agenticom workflow run <id> <task>` | Start a run |
| `agenticom workflow tools <id>` | **Show MCP tool resolution** |
| `agenticom workflow status <run-id>` | Check status |
| `agenticom workflow resume <run-id>` | Resume failed run |
| `agenticom dashboard` | **Open web UI** |
| `agenticom stats` | Show statistics |

## âš”ï¸ vs Antfarm

| Feature | Antfarm | Agenticom |
|---------|---------|-----------|
| Language | TypeScript | Python |
| Execution | Cron polling | Direct |
| **Guardrails** | âŒ | âœ… |
| **Memory** | âŒ | âœ… |
| **Approval Gates** | âŒ | âœ… |
| **Multi-Backend** | âŒ | âœ… Ollama/Claude/GPT |
| **Observability** | âŒ | âœ… Prometheus |
| **Dashboard** | âœ… | âœ… |

## ğŸ Python API

### Load & Run Workflows (Recommended)

```python
import asyncio
from orchestration import load_ready_workflow

# Load workflow with LLM executor auto-configured
team = load_ready_workflow('feature-dev.yaml')

# Execute
result = asyncio.run(team.run("Add a login button"))
print(result.final_output)
```

### Manual Setup (More Control)

```python
from orchestration import load_workflow, auto_setup_executor

# Load without executor
team = load_workflow('feature-dev.yaml')

# Configure executor manually
executor = auto_setup_executor()
for agent in team.agents.values():
    agent.set_executor(lambda p, c: executor.execute(p, c))

# Execute
result = asyncio.run(team.run("Add a login button"))
```

---

## ğŸ› ï¸ Verified Features

<details>
<summary><b>ğŸ›¡ï¸ Guardrails</b></summary>

```python
from orchestration.guardrails import ContentFilter, GuardrailPipeline

pipeline = GuardrailPipeline([
    ContentFilter(blocked_patterns=["password", r"sk-[a-zA-Z0-9]{20,}"])
])
result = pipeline.check("My password is secret")
# result[0].passed = False (blocked!)
```

</details>

<details>
<summary><b>ğŸ§  Memory</b></summary>

```python
from orchestration.memory import LocalMemoryStore

memory = LocalMemoryStore()
memory.remember("User prefers Python", tags=["preference"])
results = memory.search("Python")  # Returns matching memories
```

</details>

<details>
<summary><b>ğŸ’¾ Caching</b></summary>

```python
from orchestration.cache import LocalCache, cached

cache = LocalCache()
cache.set("key", "value", ttl=60)

@cached(ttl=300)
def expensive_llm_call(prompt):
    return llm.generate(prompt)
```

</details>

<details>
<summary><b>ğŸ“Š Observability</b></summary>

```python
from orchestration.observability import MetricsCollector

metrics = MetricsCollector()
metrics.increment("steps_total", labels={"status": "success"})
metrics.observe("step_duration", 1.5)
```

</details>

<details>
<summary><b>ğŸ”Œ MCP Tool Integration</b></summary>

Connect workflow tools to real MCP (Model Context Protocol) servers:

```python
from orchestration.tools import MCPToolBridge

# Initialize bridge
bridge = MCPToolBridge(graceful_mode=True)

# Resolve tools from workflow
tools = bridge.resolve_workflow_tools(["web_search", "literature_search"])

# Execute a tool
result = await bridge.execute("web_search", query="AI startups 2024")

# Get resolution report
report = bridge.get_resolution_report(["web_search", "market_research"])
print(report["summary"])  # {resolved: 1, fallback: 1, waiting: 0}
```

**Supported MCP Servers:**
- ğŸ“š **PubMed** - Biomedical literature search
- ğŸ” **Ahrefs** - Web search & SEO data
- ğŸ“Š **Similarweb** - Competitor traffic analysis
- ğŸ¢ **Harmonic** - Company enrichment data
- ğŸ“ˆ **Amplitude** - Product analytics
- ğŸ’¬ **LunarCrush** - Social media intelligence

</details>

<details>
<summary><b>ğŸ§  Prompt Engineering</b></summary>

Automatically improve agent prompts using Anthropic's best practices:

```python
from orchestration.tools import PromptEngineer, PromptStyle

# Initialize engineer
engineer = PromptEngineer(executor=my_llm_function)

# Improve a basic prompt
result = await engineer.improve(
    "Find papers about AI.",
    style=PromptStyle.AGENT
)
print(result.improved)  # Full structured agent prompt
print(result.improvements)  # ["Added role setting", "Added guardrails", ...]

# Generate complete agent persona
persona = await engineer.generate_agent_persona(
    role="Senior Data Analyst",
    task="Analyze customer data and identify trends",
    expertise=["Python", "SQL", "Statistics"]
)

# Sync improvement (no LLM needed - uses rule-based approach)
from orchestration.tools import improve_prompt_sync
better_prompt = improve_prompt_sync("analyze data", style=PromptStyle.ANALYSIS)
```

**Prompt Styles:** `AGENT`, `TASK`, `ANALYSIS`, `CREATIVE`, `CODING`

**Improvements Applied:**
- âœ… Role setting & expertise
- âœ… Chain-of-thought reasoning
- âœ… Output format specification
- âœ… Guardrails & safety guidelines
- âœ… Section structure

</details>

<details>
<summary><b>ğŸ“ Adaptive Memory & Lesson Learning (NEW)</b></summary>

Learn from every workflow execution and improve over time:

```python
from orchestration.lessons import LessonExtractor, LessonManager

# Extract lessons from completed workflow
extractor = LessonExtractor(llm_call=your_llm_function)
lessons = extractor.extract_from_run(
    run_id="abc123",
    workflow_id="feature-dev",
    task="Build authentication",
    status="completed",
    duration=1847.5,
    stages=workflow_stages,
    steps=workflow_steps
)

# Human curates lessons
manager = LessonManager()
for lesson in lessons:
    manager.add_proposed(lesson)  # Status: PROPOSED

# Review and approve
pending = manager.get_pending_review()
manager.approve(pending[0].id, reviewer_id="engineer", notes="Good advice")

# Retrieve relevant lessons for next workflow
lessons = manager.get_approved(
    workflow_cluster="code",
    domain_tags=["authentication", "api"]
)
```

**Monitor Memory Effectiveness:**

```python
from orchestration.memory_metrics import MemoryMetricsCollector
from orchestration.memory_config import AlertManager, get_memory_config

collector = MemoryMetricsCollector()

# Record workflow outcomes
collector.record_workflow_outcome(WorkflowOutcome(
    run_id="run-001",
    success=True,
    lessons_retrieved=["lesson-1", "lesson-2"],
    lessons_used_count=2
))

# Measure if lessons help
success_rates = collector.measure_workflow_success_rate()
print(f"Improvement: {success_rates['improvement']*100:.1f}%")
# Example: +8% (workflows with lessons are 8% more successful!)

# Automated alerting
config = get_memory_config()
alert_manager = AlertManager(config)
alerts = alert_manager.check_and_send_alerts(collector.get_dashboard_summary())
```

**Features:**
- ğŸ¯ **LLM-powered extraction** - Analyzes workflows and proposes lessons
- ğŸ‘¤ **Human curation** - Approve/reject before activation
- ğŸ“Š **Effectiveness tracking** - Measures if lessons actually help
- ğŸš¨ **Automated alerting** - Critical/Warning/Info based on thresholds
- ğŸ” **Smart filtering** - By cluster, domain tags, usage, effectiveness
- ğŸ“ˆ **Metrics dashboard** - Success rate, error reduction, satisfaction

**Configuration (Your Settings):**
- Similarity threshold: **0.80** (high quality over quantity)
- Target improvement: **5%** (realistic goal)
- Tuning frequency: **Weekly** (data-driven adjustments)
- Alert recipients: **Eng + Product** (shared ownership)

**Documentation:**
> ğŸ“– [Complete Monitoring Guide](MONITORING_ADAPTIVE_MEMORY.md) - How to measure memory effectiveness
> âš™ï¸ [Your Configuration](YOUR_MEMORY_CONFIGURATION.md) - Settings, runbooks, success criteria
> ğŸ§ª [Test Documentation](TEST_IMPLEMENTATION_COMPLETE.md) - 30+ real tests, zero mocks
> ğŸ¯ [Phase 1: Stage Tracking](PHASE1_COMPLETE_STAGE_TRACKING.md) - Workflow stage visualization

</details>

## ğŸ“ Project Structure

```
â”œâ”€â”€ agenticom/              # CLI (antfarm-style)
â”‚   â”œâ”€â”€ cli.py              # Commands
â”‚   â”œâ”€â”€ dashboard.py        # Web UI
â”‚   â”œâ”€â”€ state.py            # SQLite persistence
â”‚   â””â”€â”€ bundled_workflows/  # Ready-to-use workflows
â”‚
â”œâ”€â”€ orchestration/          # Core features
â”‚   â”œâ”€â”€ guardrails.py       # Content filtering
â”‚   â”œâ”€â”€ memory.py           # Persistent memory
â”‚   â”œâ”€â”€ lessons.py          # ğŸ†• Lesson learning system
â”‚   â”œâ”€â”€ memory_metrics.py   # ğŸ†• Memory effectiveness monitoring
â”‚   â”œâ”€â”€ memory_config.py    # ğŸ†• Adaptive memory configuration
â”‚   â”œâ”€â”€ approval.py         # Approval gates
â”‚   â”œâ”€â”€ cache.py            # Response caching
â”‚   â”œâ”€â”€ observability.py    # Metrics
â”‚   â”œâ”€â”€ integrations/       # Ollama, Claude, GPT
â”‚   â””â”€â”€ tools/              # MCP & Prompt Engineering
â”‚       â”œâ”€â”€ mcp_bridge.py   # MCP server integration
â”‚       â”œâ”€â”€ registry.py     # MCP server registry
â”‚       â”œâ”€â”€ prompt_engineer.py  # Auto prompt improvement
â”‚       â””â”€â”€ smart_refiner.py    # Multi-turn interview system
â”‚
â”œâ”€â”€ skills/                 # Assistant skills
â”‚   â”œâ”€â”€ agenticom-workflows/  # OpenClaw skill
â”‚   â””â”€â”€ agenticom-nanobot/    # Nanobot skill
â”‚
â”œâ”€â”€ tests/                  # ğŸ†• Comprehensive test suite
â”‚   â”œâ”€â”€ test_lesson_system.py   # Lesson learning tests (943 lines)
â”‚   â””â”€â”€ test_memory_metrics.py  # Memory metrics tests (784 lines)
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ TEST_RESULTS.md     # Verified test evidence
    â”œâ”€â”€ MONITORING_ADAPTIVE_MEMORY.md      # ğŸ†• Memory monitoring guide
    â”œâ”€â”€ YOUR_MEMORY_CONFIGURATION.md       # ğŸ†• Configuration & runbooks
    â”œâ”€â”€ TEST_IMPLEMENTATION_COMPLETE.md    # ğŸ†• Test documentation
    â””â”€â”€ PHASE1_COMPLETE_STAGE_TRACKING.md  # ğŸ†• Stage tracking details
```

## License

MIT

---

<p align="center">
  <strong>ğŸ• Your AI got coworkers.</strong><br>
  <a href="https://github.com/wjlgatech/agentic-company">â­ Star</a> â€¢
  <a href="https://github.com/wjlgatech/agentic-company/issues">ğŸ› Bug</a>
</p>
