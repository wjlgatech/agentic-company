{
  "run_id": "1d14e7a0",
  "artifacts": [
    {
      "type": "code",
      "filename": "output_0.py",
      "content": "from typing import List, Dict, Optional\nimport asyncio\nimport aiohttp\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport logging\nfrom abc import ABC, abstractmethod\n\n@dataclass\nclass ContentItem:\n    \"\"\"Represents a discovered content item\"\"\"\n    platform: str\n    url: str\n    title: str\n    content: str\n    author: str\n    engagement_score: float\n    relevance_score: float\n    timestamp: datetime\n    summary: str\n\nclass ContentScraperInterface(ABC):\n    \"\"\"Interface for platform-specific content scrapers\"\"\"\n    \n    @abstractmethod\n    async def scrape_content(self, limit: int = 50) -> List[ContentItem]:\n        pass\n\nclass LinkedInScraper(ContentScraperInterface):\n    \"\"\"LinkedIn content scraper implementation\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        self.base_url = \"https://api.linkedin.com/v2\"\n    \n    async def scrape_content(self, limit: int = 50) -> List[ContentItem]:\n        \"\"\"Scrape AI-related content from LinkedIn\"\"\"\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n        \n        async with aiohttp.ClientSession() as session:\n            # Search for AI-related posts\n            search_params = {\n                \"keywords\": \"artificial intelligence,AI engineering,machine learning\",\n                \"count\": limit\n            }\n            \n            async with session.get(\n                f\"{self.base_url}/posts\",\n                headers=headers,\n                params=search_params\n            ) as response:\n                data = await response.json()\n                \n                content_items = []\n                for post in data.get(\"elements\", []):\n                    item = ContentItem(\n                        platform=\"LinkedIn\",\n                        url=post.get(\"permalink\", \"\"),\n                        title=post.get(\"commentary\", \"\")[:100],\n                        content=post.get(\"commentary\", \"\"),\n                        author=post.get(\"author\", {}).get(\"name\", \"\"),\n                        engagement_score=self._calculate_engagement(post),\n                        relevance_score=0.0,  # Will be calculated later\n                        timestamp=datetime.fromtimestamp(post.get(\"createdTime\", 0) / 1000),\n                        summary=\"\"\n                    )\n                    content_items.append(item)\n                \n                return content_items\n    \n    def _calculate_engagement(self, post: Dict) -> float:\n        \"\"\"Calculate engagement score based on likes, comments, shares\"\"\"\n        likes = post.get(\"socialDetail\", {}).get(\"totalSocialActivityCounts\", {}).get(\"numLikes\", 0)\n        comments = post.get(\"socialDetail\", {}).get(\"totalSocialActivityCounts\", {}).get(\"numComments\", 0)\n        shares = post.get(\"socialDetail\", {}).get(\"totalSocialActivityCounts\", {}).get(\"numShares\", 0)\n        \n        return (likes * 1.0 + comments * 2.0 + shares * 3.0) / 100.0\n\nclass MediumScraper(ContentScraperInterface):\n    \"\"\"Medium content scraper implementation\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        self.base_url = \"https://api.medium.com/v1\"\n    \n    async def scrape_content(self, limit: int = 50) -> List[ContentItem]:\n        \"\"\"Scrape AI-related content from Medium\"\"\"\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n        \n        async with aiohttp.ClientSession() as session:\n            # Search for AI-related articles\n            search_params = {\n                \"tag\": \"artificial-intelligence\",\n                \"limit\": limit\n            }\n            \n            async with session.get(\n                f\"{self.base_url}/posts\",\n                headers=headers,\n                params=search_params\n            ) as response:\n                data = await response.json()\n                \n                content_items = []\n                for article in data.get(\"data\", []):\n                    item = ContentItem(\n                        platform=\"Medium\",\n                        url=article.get(\"url\", \"\"),\n                        title=article.get(\"title\", \"\"),\n                        content=article.get(\"content\", \"\"),\n                        author=article.get(\"author\", {}).get(\"name\", \"\"),\n                        engagement_score=self._calculate_engagement(article),\n                        relevance_score=0.0,\n                        timestamp=datetime.fromisoformat(article.get(\"publishedAt\", \"\")),\n                        summary=\"\"\n                    )\n                    content_items.append(item)\n                \n                return content_items\n    \n    def _calculate_engagement(self, article: Dict) -> float:\n        \"\"\"Calculate engagement score for Medium article\"\"\"\n        claps = article.get(\"virtuals\", {}).get(\"totalClapCount\", 0)\n        responses = article.get(\"virtuals\", {}).get(\"responsesCreatedCount\", 0)\n        \n        return (claps * 0.1 + responses * 5.0) / 100.0\n\nclass FacebookScraper(ContentScraperInterface):\n    \"\"\"Facebook content scraper implementation\"\"\"\n    \n    def __init__(self, access_token: str):\n        self.access_token = access_token\n        self.base_url = \"https://graph.facebook.com/v18.0\"\n    \n    async def scrape_content(self, limit: int = 50) -> List[ContentItem]:\n        \"\"\"Scrape AI-related content from Facebook\"\"\"\n        params = {\n            \"access_token\": self.access_token,\n            \"q\": \"artificial intelligence AI technology\",\n            \"type\": \"post\",\n            \"limit\": limit\n        }\n        \n        async with aiohttp.ClientSession() as session:\n            async with session.get(\n                f\"{self.base_url}/search\",\n                params=params\n            ) as response:\n                data = await response.json()\n                \n                content_items = []\n                for post in data.get(\"data\", []):\n                    item = ContentItem(\n                        platform=\"Facebook\",\n                        url=f\"https://facebook.com/{post.get('id', '')}\",\n                        title=post.get(\"message\", \"\")[:100],\n                        content=post.get(\"message\", \"\"),\n                        author=post.get(\"from\", {}).get(\"name\", \"\"),\n                        engagement_score=self._calculate_engagement(post),\n                        relevance_score=0.0,\n                        timestamp=datetime.fromisoformat(post.get(\"created_time\", \"\")),\n                        summary=\"\"\n                    )\n                    content_items.append(item)\n                \n                return content_items\n    \n    def _calculate_engagement(self, post: Dict) -> float:\n        \"\"\"Calculate engagement score for Facebook post\"\"\"\n        reactions = post.get(\"reactions\", {}).get(\"summary\", {}).get(\"total_count\", 0)\n        comments = post.get(\"comments\", {}).get(\"summary\", {}).get(\"total_count\", 0)\n        shares = post.get(\"shares\", {}).get(\"count\", 0)\n        \n        return (reactions * 1.0 + comments * 2.0 + shares * 3.0) / 100.0\n\nclass RelevanceScorer:\n    \"\"\"Scores content relevance based on AI engineering, product design, business wealth, individual empowerment\"\"\"\n    \n    RELEVANCE_KEYWORDS = {\n        \"ai_engineering\": [\n            \"machine learning\", \"deep learning\", \"neural networks\", \"AI development\",\n            \"MLOps\", \"model training\", \"algorithms\", \"data science\", \"AI architecture\"\n        ],\n        \"product_design\": [\n            \"UX design\", \"product management\", \"user experience\", \"design thinking\",\n            \"product strategy\", \"user research\", \"prototyping\", \"design systems\"\n        ],\n        \"business_wealth\": [\n            \"entrepreneurship\", \"startup\", \"business growth\", \"revenue\", \"investment\",\n            \"wealth building\", \"financial success\", \"business strategy\", \"scaling\"\n        ],\n        \"individual_empowerment\": [\n            \"personal development\", \"career growth\", \"skill building\", \"leadership\",\n            \"productivity\", \"self-improvement\", \"empowerment\", \"success mindset\"\n        ]\n    }\n    \n    def score_relevance(self, content: ContentItem) -> float:\n        \"\"\"Calculate relevance score based on content analysis\"\"\"\n        text = f\"{content.title} {content.content}\".lower()\n        \n        total_score = 0.0\n        for category, keywords in self.RELEVANCE_KEYWORDS.items():\n            category_score = sum(1 for keyword in keywords if keyword in text)\n            total_score += category_score\n        \n        # Normalize score to 0-1 range\n        max_possible_score = sum(len(keywords) for keywords in self.RELEVANCE_KEYWORDS.values())\n        return min(total_score / max_possible_score, 1.0)\n\nclass ContentSummarizer:\n    \"\"\"Generates concise summaries of content items\"\"\"\n    \n    async def summarize(self, content: ContentItem) -> str:\n        \"\"\"Generate a summary of the content item\"\"\"\n        # In a real implementation, this would use an AI summarization service\n        # For now, we'll create a simple extractive summary\n        \n        sentences = content.content.split('. ')\n        if len(sentences) <= 2:\n            return content.content\n        \n        # Take first and last sentences as summary\n        summary = f\"{sentences[0]}. {sentences[-1]}\"\n        return summary[:200] + \"...\" if len(summary) > 200 else summary\n\nclass ContentDiscoveryAgent:\n    \"\"\"Main agent for content discovery and curation\"\"\"\n    \n    def __init__(self, linkedin_api_key: str, medium_api_key: str, facebook_access_token: str):\n        self.scrapers = [\n            LinkedInScraper(linkedin_api_key),\n            MediumScraper(medium_api_key),\n            FacebookScraper(facebook_access_token)\n        ]\n        self.relevance_scorer = RelevanceScorer()\n        self.summarizer = ContentSummarizer()\n        self.logger = logging.getLogger(__name__)\n    \n    async def discover_content(self, daily_limit: int = 100) -> List[ContentItem]:\n        \"\"\"Discover and curate content from all platforms\"\"\"\n        self.logger.info(f\"Starting content discovery with limit: {daily_limit}\")\n        \n        # Distribute limit across platforms\n        per_platform_limit = daily_limit // len(self.scrapers)\n        \n        all_content = []\n        tasks = []\n        \n        for scraper in self.scrapers:\n            task = scraper.scrape_content(per_platform_limit)\n            tasks.append(task)\n        \n        # Execute scraping in parallel\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        for result in results:\n            if isinstance(result, Exception):\n                self.logger.error(f\"Scraping error: {result}\")\n                continue\n            all_content.extend(result)\n        \n        # Score relevance and generate summaries\n        for item in all_content:\n            item.relevance_score = self.relevance_scorer.score_relevance(item)\n            item.summary = await self.summarizer.summarize(item)\n        \n        # Filter and sort by combined score\n        filtered_content = [item for item in all_content if item.relevance_score > 0.3]\n        \n        # Sort by combined relevance and engagement score\n        sorted_content = sorted(\n            filtered_content,\n            key=lambda x: (x.relevance_score * 0.7 + x.engagement_score * 0.3),\n            reverse=True\n        )\n        \n        self.logger.info(f\"Discovered {len(sorted_content)} relevant content items\")\n        return sorted_content[:5]  # Return top 5 candidates\n    \n    async def present_candidates(self, candidates: List[ContentItem]) -> Dict:\n        \"\"\"Present top candidates in a format ready for client review\"\"\"\n        presentation = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"total_candidates\": len(candidates),\n            \"candidates\": []\n        }\n        \n        for i, candidate in enumerate(candidates):\n            candidate_data = {\n                \"rank\": i + 1,\n                \"platform\": candidate.platform,\n                \"title\": candidate.title,\n                \"author\": candidate.author,\n                \"summary\": candidate.summary,\n                \"relevance_score\": round(candidate.relevance_score, 2),\n                \"engagement_score\": round(candidate.engagement_score, 2),\n                \"url\": candidate.url,\n                \"timestamp\": candidate.timestamp.isoformat()\n            }\n            presentation[\"candidates\"].append(candidate_data)\n        \n        return presentation",
      "language": "python",
      "metadata": {
        "extracted_from": "code_block",
        "index": 0
      },
      "created_at": "2026-02-14T02:04:02.469365"
    }
  ],
  "created_at": "2026-02-14T02:04:02.469420",
  "metadata": {},
  "stats": {
    "count": 1,
    "total_size_bytes": 12450,
    "total_lines": 303
  }
}